"""
AIæ¨¡å‹æ¨¡å— - ä½¿ç”¨CLIPã€YOLOç­‰æ¨¡å‹è¿›è¡Œæ±½è½¦å›¾ç‰‡æ™ºèƒ½æ ‡æ³¨
"""
import torch
import clip
import cv2
import numpy as np
from PIL import Image
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import json
from tqdm import tqdm
import pandas as pd

from config import MODEL_CONFIG, LABEL_CONFIG, DATA_CONFIG

class CarImageTagger:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–CLIPæ¨¡å‹
        self.clip_model, self.clip_preprocess = self._load_clip_model()
        
        # å®šä¹‰æ±½è½¦ç›¸å…³çš„æ–‡æœ¬æç¤º
        self.car_prompts = self._create_car_prompts()
        
    def _load_clip_model(self):
        """åŠ è½½CLIPæ¨¡å‹"""
        print("ğŸ“¥ åŠ è½½CLIPæ¨¡å‹...")
        model, preprocess = clip.load(MODEL_CONFIG["clip"]["model_name"], device=self.device)
        return model, preprocess
    
    def _create_car_prompts(self) -> Dict[str, List[str]]:
        """åˆ›å»ºæ±½è½¦ç›¸å…³çš„æ–‡æœ¬æç¤º"""
        prompts = {
            "angles": [
                "front view of a car", "rear view of a car", "side view of a car",
                "45 degree front angle of a car", "45 degree rear angle of a car",
                "car interior", "car dashboard", "car steering wheel", "car seats",
                "car headlights", "car taillights", "car grille", "car wheels",
                "car spoiler", "car console", "car door panel", "car sunroof",
                "car trunk", "car front trunk", "car air vents", "car instrument panel",
                "car diffuser", "car C-pillar", "car charging port"
            ],
            "brands": [
                "Cadillac car", "Ferrari car", "Honda car", "MINI car", 
                "Nissan car", "Porsche car", "Smart car", "Toyota car"
            ],
            "styles": [
                "electric car", "hybrid car", "sports car", "luxury car", 
                "concept car", "vintage car", "modern car", "classic car",
                "business car", "family car", "off-road car", "racing car",
                "SUV car", "sedan car", "hatchback car", "convertible car"
            ],
            "colors": [
                "black car", "white car", "silver car", "gray car", "red car",
                "blue car", "green car", "yellow car", "orange car", "purple car",
                "brown car", "gold car", "champagne car", "pearl white car"
            ]
        }
        return prompts
    
    def classify_with_clip(self, image_path: str) -> Dict[str, float]:
        """ä½¿ç”¨CLIPå¯¹å›¾ç‰‡è¿›è¡Œåˆ†ç±»"""
        try:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
            image = Image.open(image_path).convert('RGB')
            image_input = self.clip_preprocess(image).unsqueeze(0).to(self.device)
            
            results = {}
            
            # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œåˆ†ç±»
            for category, prompts in self.car_prompts.items():
                # å‡†å¤‡æ–‡æœ¬è¾“å…¥
                text_inputs = clip.tokenize(prompts).to(self.device)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                with torch.no_grad():
                    image_features = self.clip_model.encode_image(image_input)
                    text_features = self.clip_model.encode_text(text_inputs)
                    
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
                    
                    # è·å–æœ€é«˜åˆ†çš„ç»“æœ
                    values, indices = similarity[0].topk(3)
                    
                    # ä¿å­˜ç»“æœ
                    category_results = {}
                    for i, (value, idx) in enumerate(zip(values, indices)):
                        category_results[prompts[idx]] = float(value)
                    
                    results[category] = category_results
            
            return results
            
        except Exception as e:
            print(f"âŒ CLIPåˆ†ç±»å¤±è´¥: {image_path}, é”™è¯¯: {e}")
            return {}
    
    def extract_angle_from_clip(self, clip_results: Dict) -> Tuple[str, float]:
        """ä»CLIPç»“æœä¸­æå–è§’åº¦ä¿¡æ¯"""
        if "angles" not in clip_results:
            return "Unknown", 0.0
        
        angle_scores = clip_results["angles"]
        
        # è§’åº¦æ˜ å°„
        angle_mapping = {
            "front view of a car": "4-æ­£å‰",
            "rear view of a car": "5-æ­£å", 
            "side view of a car": "2-æ­£ä¾§",
            "45 degree front angle of a car": "1-å‰45",
            "45 degree rear angle of a car": "3-å45",
            "car interior": "10-å†…é¥°",
            "car dashboard": "10-å†…é¥°",
            "car steering wheel": "11-æ–¹å‘ç›˜",
            "car seats": "14-åº§æ¤…",
            "car headlights": "6-å¤´ç¯",
            "car taillights": "7-å°¾ç¯",
            "car grille": "8-æ ¼æ …",
            "car wheels": "8-è½®æ¯‚",
            "car spoiler": "9-å°¾ç¿¼",
            "car console": "13-CONSOLE",
            "car door panel": "15-é—¨æ¿",
            "car sunroof": "17-å¤©çª—",
            "car trunk": "18-åå¤‡ç®±",
            "car front trunk": "19-å‰å¤‡ç®±",
            "car air vents": "20-å‡ºé£å£",
            "car instrument panel": "21-ä»ªè¡¨å±",
            "car diffuser": "22-æ‰©æ•£å™¨",
            "car C-pillar": "23-CæŸ±",
            "car charging port": "24-å……ç”µå£"
        }
        
        best_angle = "Unknown"
        best_score = 0.0
        
        for prompt, score in angle_scores.items():
            if prompt in angle_mapping and score > best_score:
                best_angle = angle_mapping[prompt]
                best_score = score
        
        return best_angle, best_score
    
    def extract_brand_from_clip(self, clip_results: Dict) -> Tuple[str, float]:
        """ä»CLIPç»“æœä¸­æå–å“ç‰Œä¿¡æ¯"""
        if "brands" not in clip_results:
            return "Unknown", 0.0
        
        brand_scores = clip_results["brands"]
        
        # å“ç‰Œæ˜ å°„
        brand_mapping = {
            "Cadillac car": "Cadillac",
            "Ferrari car": "Ferrari", 
            "Honda car": "Honda",
            "MINI car": "MINI",
            "Nissan car": "Nissan",
            "Porsche car": "Porsche",
            "Smart car": "Smart",
            "Toyota car": "Toyota"
        }
        
        best_brand = "Unknown"
        best_score = 0.0
        
        for prompt, score in brand_scores.items():
            if prompt in brand_mapping and score > best_score:
                best_brand = brand_mapping[prompt]
                best_score = score
        
        return best_brand, best_score
    
    def extract_style_from_clip(self, clip_results: Dict) -> Tuple[str, float]:
        """ä»CLIPç»“æœä¸­æå–é£æ ¼ä¿¡æ¯"""
        if "styles" not in clip_results:
            return "Unknown", 0.0
        
        style_scores = clip_results["styles"]
        
        # é£æ ¼æ˜ å°„
        style_mapping = {
            "electric car": "æ–°èƒ½æº",
            "hybrid car": "æ–°èƒ½æº",
            "sports car": "è¿åŠ¨",
            "luxury car": "è±ªå",
            "concept car": "æ¦‚å¿µè½¦",
            "vintage car": "å¤å¤",
            "modern car": "ç°ä»£",
            "classic car": "ç»å…¸",
            "business car": "å•†åŠ¡",
            "family car": "å®¶ç”¨",
            "off-road car": "è¶Šé‡",
            "racing car": "è·‘è½¦",
            "SUV car": "SUV",
            "sedan car": "è½¿è½¦",
            "hatchback car": "æ€èƒŒè½¦",
            "convertible car": "æ•ç¯·è½¦"
        }
        
        best_style = "Unknown"
        best_score = 0.0
        
        for prompt, score in style_scores.items():
            if prompt in style_mapping and score > best_score:
                best_style = style_mapping[prompt]
                best_score = score
        
        return best_style, best_score
    
    def process_single_image(self, image_path: str) -> Dict:
        """å¤„ç†å•å¼ å›¾ç‰‡ï¼Œè¿”å›å®Œæ•´çš„æ ‡æ³¨ä¿¡æ¯"""
        print(f"ğŸ” å¤„ç†å›¾ç‰‡: {Path(image_path).name}")
        
        # ä½¿ç”¨CLIPè¿›è¡Œåˆ†ç±»
        clip_results = self.classify_with_clip(image_path)
        
        # æå–å„ç§ä¿¡æ¯
        angle, angle_confidence = self.extract_angle_from_clip(clip_results)
        brand, brand_confidence = self.extract_brand_from_clip(clip_results)
        style, style_confidence = self.extract_style_from_clip(clip_results)
        
        # è·å–å›¾ç‰‡åŸºæœ¬ä¿¡æ¯
        try:
            img = Image.open(image_path)
            width, height = img.size
            file_size = Path(image_path).stat().st_size
        except:
            width, height, file_size = 0, 0, 0
        
        # æ„å»ºç»“æœ
        result = {
            "image_path": str(image_path),
            "image_id": f"auto_{Path(image_path).stem}",
            "source": "brand_images",
            "brand": brand,
            "brand_confidence": brand_confidence,
            "angle": angle,
            "angle_confidence": angle_confidence,
            "style": style,
            "style_confidence": style_confidence,
            "width": width,
            "height": height,
            "file_size": file_size,
            "needs_annotation": True,
            "auto_tags": [angle, brand, style],
            "manual_tags": [],
            "confidence": (angle_confidence + brand_confidence + style_confidence) / 3,
            "clip_results": clip_results
        }
        
        return result
    
    def process_brand_images(self, brand_path: Path, max_images: int = 100) -> List[Dict]:
        """å¤„ç†æŸä¸ªå“ç‰Œçš„æ‰€æœ‰å›¾ç‰‡"""
        print(f"ğŸš— å¤„ç†å“ç‰Œå›¾ç‰‡: {brand_path.name}")
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png']:
            image_files.extend(list(brand_path.rglob(ext)))
        
        # é™åˆ¶å¤„ç†æ•°é‡
        if max_images and len(image_files) > max_images:
            image_files = image_files[:max_images]
            print(f"  ğŸ“Š é™åˆ¶å¤„ç†æ•°é‡: {max_images} å¼ ")
        
        results = []
        for img_path in tqdm(image_files, desc=f"å¤„ç† {brand_path.name}"):
            try:
                result = self.process_single_image(str(img_path))
                results.append(result)
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {img_path}, é”™è¯¯: {e}")
        
        return results
    
    def process_all_brands(self, max_images_per_brand: int = 50) -> pd.DataFrame:
        """å¤„ç†æ‰€æœ‰å“ç‰Œçš„å›¾ç‰‡"""
        print("ğŸš€ å¼€å§‹å¤„ç†æ‰€æœ‰å“ç‰Œå›¾ç‰‡...")
        
        all_results = []
        brand_images_path = DATA_CONFIG["brand_images"]
        
        for brand in LABEL_CONFIG["brands"]:
            brand_path = brand_images_path / brand
            if brand_path.exists():
                brand_results = self.process_brand_images(brand_path, max_images_per_brand)
                all_results.extend(brand_results)
                print(f"  âœ… {brand}: å¤„ç†äº† {len(brand_results)} å¼ å›¾ç‰‡")
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(all_results)
        
        # ä¿å­˜ç»“æœ
        output_path = DATA_CONFIG["processed_data"] / "auto_annotated_dataset.csv"
        df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"âœ… è‡ªåŠ¨æ ‡æ³¨æ•°æ®é›†å·²ä¿å­˜: {len(df)} æ¡è®°å½•")
        print(f"ğŸ“ ä¿å­˜è·¯å¾„: {output_path}")
        
        return df

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºAIæ ‡æ³¨åŠŸèƒ½"""
    print("ğŸ¤– å¯åŠ¨æ±½è½¦å›¾ç‰‡AIæ ‡æ³¨ç³»ç»Ÿ...")
    
    # åˆå§‹åŒ–æ ‡æ³¨å™¨
    tagger = CarImageTagger()
    
    # å¤„ç†æ‰€æœ‰å“ç‰Œå›¾ç‰‡ï¼ˆé™åˆ¶æ•°é‡ä»¥èŠ‚çœæ—¶é—´ï¼‰
    df = tagger.process_all_brands(max_images_per_brand=20)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ ‡æ³¨ç»“æœç»Ÿè®¡:")
    print(f"  æ€»å›¾ç‰‡æ•°: {len(df)}")
    print(f"  å“ç‰Œåˆ†å¸ƒ: {df['brand'].value_counts().to_dict()}")
    print(f"  è§’åº¦åˆ†å¸ƒ: {df['angle'].value_counts().to_dict()}")
    print(f"  é£æ ¼åˆ†å¸ƒ: {df['style'].value_counts().to_dict()}")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {df['confidence'].mean():.3f}")

if __name__ == "__main__":
    main()
