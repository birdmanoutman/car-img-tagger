#!/usr/bin/env python3
"""
MySQLæ•°æ®åº“å¤‡ä»½å·¥å…·
æ”¯æŒå®Œæ•´å¤‡ä»½ã€å¢é‡å¤‡ä»½ã€å‹ç¼©å¤‡ä»½ç­‰åŠŸèƒ½
"""

import os
import sys
import json
import gzip
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root / "src"))

from car_img_tagger.config import DATABASE_CONFIG, DATA_CONFIG
import pymysql
from pymysql.cursors import DictCursor

class DatabaseBackup:
    """æ•°æ®åº“å¤‡ä»½ç±»"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or DATABASE_CONFIG["mysql"]
        self.backup_dir = DATA_CONFIG["output"] / "database_backups"
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return pymysql.connect(
            host=self.config['host'],
            port=self.config['port'],
            user=self.config['user'],
            password=self.config['password'],
            database=self.config['database'],
            charset=self.config['charset'],
            cursorclass=DictCursor,
            autocommit=True
        )
    
    def get_database_info(self) -> Dict:
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            # è·å–æ•°æ®åº“ç‰ˆæœ¬
            cursor.execute('SELECT VERSION() as version')
            version_result = cursor.fetchone()
            
            # è·å–æ•°æ®åº“å¤§å°
            cursor.execute(f'''
                SELECT 
                    ROUND(SUM(data_length + index_length) / 1024 / 1024, 2) AS size_mb
                FROM information_schema.tables 
                WHERE table_schema = '{self.config['database']}'
            ''')
            size_result = cursor.fetchone()
            
            # è·å–è¡¨ä¿¡æ¯
            cursor.execute(f'''
                SELECT 
                    table_name,
                    table_rows,
                    ROUND((data_length + index_length) / 1024 / 1024, 2) AS size_mb
                FROM information_schema.tables 
                WHERE table_schema = '{self.config['database']}'
                ORDER BY size_mb DESC
            ''')
            tables_result = cursor.fetchall()
            
            return {
                'version': version_result['version'],
                'total_size_mb': size_result['size_mb'] or 0,
                'tables': [
                    {
                        'name': row.get('table_name', row.get('TABLE_NAME', '')),
                        'rows': row.get('table_rows', row.get('TABLE_ROWS', 0)),
                        'size_mb': row.get('size_mb', row.get('SIZE_MB', 0))
                    }
                    for row in tables_result
                ]
            }
    
    def export_table_data(self, table_name: str) -> List[Dict]:
        """å¯¼å‡ºè¡¨æ•°æ®"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(f'SELECT * FROM {table_name}')
            return cursor.fetchall()
    
    def export_table_structure(self, table_name: str) -> str:
        """å¯¼å‡ºè¡¨ç»“æ„"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(f'SHOW CREATE TABLE {table_name}')
            result = cursor.fetchone()
            return result[f'Create Table']
    
    def create_full_backup(self, compress: bool = True) -> str:
        """åˆ›å»ºå®Œæ•´å¤‡ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"full_backup_{timestamp}"
        backup_path = self.backup_dir / backup_name
        
        print(f"åˆ›å»ºå®Œæ•´å¤‡ä»½: {backup_name}")
        
        # è·å–æ•°æ®åº“ä¿¡æ¯
        db_info = self.get_database_info()
        print(f"æ•°æ®åº“ä¿¡æ¯:")
        print(f"  ç‰ˆæœ¬: {db_info['version']}")
        print(f"  æ€»å¤§å°: {db_info['total_size_mb']} MB")
        print(f"  è¡¨æ•°é‡: {len(db_info['tables'])}")
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_path.mkdir(exist_ok=True)
        
        # å¤‡ä»½å…ƒæ•°æ®
        metadata = {
            'backup_type': 'full',
            'backup_time': datetime.now().isoformat(),
            'database_info': db_info,
            'config': {
                'host': self.config['host'],
                'database': self.config['database'],
                'charset': self.config['charset']
            }
        }
        
        with open(backup_path / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
        
        # å¤‡ä»½æ¯ä¸ªè¡¨
        for table_info in db_info['tables']:
            table_name = table_info['name']
            print(f"å¤‡ä»½è¡¨: {table_name} ({table_info['rows']} è¡Œ)")
            
            # å¤‡ä»½è¡¨ç»“æ„
            structure = self.export_table_structure(table_name)
            structure_file = backup_path / f"{table_name}_structure.sql"
            with open(structure_file, 'w', encoding='utf-8') as f:
                f.write(structure)
            
            # å¤‡ä»½è¡¨æ•°æ®
            if table_info['rows'] > 0:
                data = self.export_table_data(table_name)
                data_file = backup_path / f"{table_name}_data.json"
                
                with open(data_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        
        # å‹ç¼©å¤‡ä»½
        if compress:
            print("å‹ç¼©å¤‡ä»½æ–‡ä»¶...")
            compressed_path = f"{backup_path}.tar.gz"
            
            import tarfile
            with tarfile.open(compressed_path, 'w:gz') as tar:
                tar.add(backup_path, arcname=backup_name)
            
            # åˆ é™¤æœªå‹ç¼©çš„ç›®å½•
            shutil.rmtree(backup_path)
            backup_path = Path(compressed_path)
        
        print(f"å¤‡ä»½å®Œæˆ: {backup_path}")
        return str(backup_path)
    
    def create_incremental_backup(self, last_backup_time: str) -> str:
        """åˆ›å»ºå¢é‡å¤‡ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"incremental_backup_{timestamp}"
        backup_path = self.backup_dir / backup_name
        
        print(f"ğŸ”„ åˆ›å»ºå¢é‡å¤‡ä»½: {backup_name}")
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_path.mkdir(exist_ok=True)
        
        # å¤‡ä»½å…ƒæ•°æ®
        metadata = {
            'backup_type': 'incremental',
            'backup_time': datetime.now().isoformat(),
            'last_backup_time': last_backup_time,
            'config': {
                'host': self.config['host'],
                'database': self.config['database'],
                'charset': self.config['charset']
            }
        }
        
        with open(backup_path / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
        
        # è·å–è‡ªä¸Šæ¬¡å¤‡ä»½ä»¥æ¥çš„å˜æ›´
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            # è·å–æ‰€æœ‰è¡¨
            cursor.execute(f'''
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = '{self.config['database']}'
            ''')
            tables = cursor.fetchall()
            
            for table in tables:
                table_name = table['table_name']
                
                # æ£€æŸ¥è¡¨æ˜¯å¦æœ‰æ—¶é—´æˆ³å­—æ®µ
                cursor.execute(f'''
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_schema = '{self.config['database']}' 
                    AND table_name = '{table_name}' 
                    AND column_name IN ('created_at', 'updated_at', 'modified_at')
                ''')
                timestamp_columns = cursor.fetchall()
                
                if timestamp_columns:
                    # æœ‰æ—¶é—´æˆ³å­—æ®µï¼Œåªå¤‡ä»½å˜æ›´çš„æ•°æ®
                    timestamp_col = timestamp_columns[0]['column_name']
                    cursor.execute(f'''
                        SELECT * FROM {table_name} 
                        WHERE {timestamp_col} > %s
                    ''', (last_backup_time,))
                    
                    changed_data = cursor.fetchall()
                    if changed_data:
                        print(f"ğŸ“‹ å¤‡ä»½è¡¨ {table_name} çš„å˜æ›´: {len(changed_data)} è¡Œ")
                        
                        data_file = backup_path / f"{table_name}_changes.json"
                        with open(data_file, 'w', encoding='utf-8') as f:
                            json.dump(changed_data, f, ensure_ascii=False, indent=2, default=str)
                else:
                    # æ²¡æœ‰æ—¶é—´æˆ³å­—æ®µï¼Œå¤‡ä»½æ•´ä¸ªè¡¨
                    print(f"ğŸ“‹ å¤‡ä»½è¡¨ {table_name} (æ— æ—¶é—´æˆ³å­—æ®µ)")
                    data = self.export_table_data(table_name)
                    data_file = backup_path / f"{table_name}_data.json"
                    
                    with open(data_file, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… å¢é‡å¤‡ä»½å®Œæˆ: {backup_path}")
        return str(backup_path)
    
    def list_backups(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []
        
        for backup_path in self.backup_dir.iterdir():
            if backup_path.is_dir():
                metadata_file = backup_path / "metadata.json"
                if metadata_file.exists():
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    backups.append({
                        'name': backup_path.name,
                        'path': str(backup_path),
                        'type': metadata.get('backup_type', 'unknown'),
                        'time': metadata.get('backup_time', 'unknown'),
                        'size': self._get_directory_size(backup_path)
                    })
            elif backup_path.suffix == '.gz':
                # å‹ç¼©å¤‡ä»½
                backups.append({
                    'name': backup_path.stem,
                    'path': str(backup_path),
                    'type': 'compressed',
                    'time': datetime.fromtimestamp(backup_path.stat().st_mtime).isoformat(),
                    'size': backup_path.stat().st_size
                })
        
        # æŒ‰æ—¶é—´æ’åº
        backups.sort(key=lambda x: x['time'], reverse=True)
        return backups
    
    def _get_directory_size(self, path: Path) -> int:
        """è·å–ç›®å½•å¤§å°"""
        total_size = 0
        for file_path in path.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size
    
    def restore_backup(self, backup_path: str) -> bool:
        """æ¢å¤å¤‡ä»½"""
        print(f"ğŸ”„ æ¢å¤å¤‡ä»½: {backup_path}")
        
        backup_path = Path(backup_path)
        
        # å¦‚æœæ˜¯å‹ç¼©å¤‡ä»½ï¼Œå…ˆè§£å‹
        if backup_path.suffix == '.gz':
            print("ğŸ“¦ è§£å‹å¤‡ä»½æ–‡ä»¶...")
            import tarfile
            extract_dir = backup_path.parent / backup_path.stem
            with tarfile.open(backup_path, 'r:gz') as tar:
                tar.extractall(extract_dir)
            backup_path = extract_dir
        
        # è¯»å–å…ƒæ•°æ®
        metadata_file = backup_path / "metadata.json"
        if not metadata_file.exists():
            print("âŒ å¤‡ä»½å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        print(f"ğŸ“‹ å¤‡ä»½ä¿¡æ¯:")
        print(f"  ç±»å‹: {metadata['backup_type']}")
        print(f"  æ—¶é—´: {metadata['backup_time']}")
        
        # æ¢å¤è¡¨ç»“æ„
        print("ğŸ—ï¸ æ¢å¤è¡¨ç»“æ„...")
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            # ç¦ç”¨å¤–é”®æ£€æŸ¥
            cursor.execute('SET FOREIGN_KEY_CHECKS = 0')
            
            # åˆ é™¤ç°æœ‰è¡¨
            for table_info in metadata['database_info']['tables']:
                table_name = table_info['name']
                cursor.execute(f'DROP TABLE IF EXISTS {table_name}')
                print(f"ğŸ—‘ï¸ åˆ é™¤è¡¨: {table_name}")
            
            # é‡æ–°åˆ›å»ºè¡¨
            for table_info in metadata['database_info']['tables']:
                table_name = table_info['name']
                structure_file = backup_path / f"{table_name}_structure.sql"
                
                if structure_file.exists():
                    with open(structure_file, 'r', encoding='utf-8') as f:
                        create_sql = f.read()
                    
                    cursor.execute(create_sql)
                    print(f"âœ… åˆ›å»ºè¡¨: {table_name}")
            
            # æ¢å¤è¡¨æ•°æ®
            print("ğŸ“Š æ¢å¤è¡¨æ•°æ®...")
            for table_info in metadata['database_info']['tables']:
                table_name = table_info['name']
                data_file = backup_path / f"{table_name}_data.json"
                
                if data_file.exists():
                    with open(data_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if data:
                        # æ„å»ºæ’å…¥SQL
                        columns = list(data[0].keys())
                        placeholders = ', '.join(['%s'] * len(columns))
                        insert_sql = f'''
                            INSERT INTO {table_name} ({', '.join(columns)}) 
                            VALUES ({placeholders})
                        '''
                        
                        # æ‰¹é‡æ’å…¥æ•°æ®
                        cursor.executemany(insert_sql, [
                            [row[col] for col in columns] for row in data
                        ])
                        print(f"âœ… æ¢å¤è¡¨ {table_name}: {len(data)} è¡Œ")
            
            # é‡æ–°å¯ç”¨å¤–é”®æ£€æŸ¥
            cursor.execute('SET FOREIGN_KEY_CHECKS = 1')
            conn.commit()
        
        print("âœ… å¤‡ä»½æ¢å¤å®Œæˆ")
        return True
    
    def cleanup_old_backups(self, keep_days: int = 30):
        """æ¸…ç†æ—§å¤‡ä»½"""
        print(f"ğŸ§¹ æ¸…ç† {keep_days} å¤©å‰çš„å¤‡ä»½...")
        
        cutoff_time = datetime.now().timestamp() - (keep_days * 24 * 60 * 60)
        deleted_count = 0
        
        for backup_path in self.backup_dir.iterdir():
            if backup_path.stat().st_mtime < cutoff_time:
                try:
                    if backup_path.is_dir():
                        shutil.rmtree(backup_path)
                    else:
                        backup_path.unlink()
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§å¤‡ä»½: {backup_path.name}")
                    deleted_count += 1
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤å¤‡ä»½å¤±è´¥ {backup_path.name}: {e}")
        
        print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§å¤‡ä»½")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="MySQLæ•°æ®åº“å¤‡ä»½å·¥å…·")
    parser.add_argument("--action", choices=['backup', 'restore', 'list', 'info', 'cleanup'], 
                       default='backup', help="æ“ä½œç±»å‹")
    parser.add_argument("--type", choices=['full', 'incremental'], 
                       default='full', help="å¤‡ä»½ç±»å‹")
    parser.add_argument("--compress", action='store_true', help="å‹ç¼©å¤‡ä»½")
    parser.add_argument("--backup-path", help="å¤‡ä»½æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ¢å¤ï¼‰")
    parser.add_argument("--keep-days", type=int, default=30, help="ä¿ç•™å¤‡ä»½å¤©æ•°")
    
    args = parser.parse_args()
    
    try:
        backup = DatabaseBackup()
        
        if args.action == 'backup':
            if args.type == 'full':
                backup_path = backup.create_full_backup(args.compress)
                print(f"ğŸ‰ å®Œæ•´å¤‡ä»½å®Œæˆ: {backup_path}")
            else:
                # å¢é‡å¤‡ä»½éœ€è¦æŒ‡å®šä¸Šæ¬¡å¤‡ä»½æ—¶é—´
                print("âš ï¸ å¢é‡å¤‡ä»½éœ€è¦æŒ‡å®šä¸Šæ¬¡å¤‡ä»½æ—¶é—´")
                return 1
        
        elif args.action == 'restore':
            if not args.backup_path:
                print("âŒ æ¢å¤å¤‡ä»½éœ€è¦æŒ‡å®šå¤‡ä»½è·¯å¾„")
                return 1
            
            success = backup.restore_backup(args.backup_path)
            if success:
                print("ğŸ‰ å¤‡ä»½æ¢å¤å®Œæˆ")
            else:
                print("âŒ å¤‡ä»½æ¢å¤å¤±è´¥")
                return 1
        
        elif args.action == 'list':
            backups = backup.list_backups()
            print(f"ğŸ“‹ å¤‡ä»½åˆ—è¡¨ (å…± {len(backups)} ä¸ª):")
            for backup_info in backups:
                size_mb = backup_info['size'] / 1024 / 1024
                print(f"  {backup_info['name']} - {backup_info['type']} - {size_mb:.2f} MB - {backup_info['time']}")
        
        elif args.action == 'info':
            db_info = backup.get_database_info()
            print(f"ğŸ“Š æ•°æ®åº“ä¿¡æ¯:")
            print(f"  ç‰ˆæœ¬: {db_info['version']}")
            print(f"  æ€»å¤§å°: {db_info['total_size_mb']} MB")
            print(f"  è¡¨æ•°é‡: {len(db_info['tables'])}")
            print(f"  è¡¨è¯¦æƒ…:")
            for table in db_info['tables']:
                print(f"    {table['name']}: {table['rows']} è¡Œ, {table['size_mb']} MB")
        
        elif args.action == 'cleanup':
            backup.cleanup_old_backups(args.keep_days)
        
        return 0
        
    except Exception as e:
        print(f"æ“ä½œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
